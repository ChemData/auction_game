New training data is generated from each completed Auction game. Part of this process involves determining the mean and standard deviation price that the NN should have chosen for a particular good. This is done by computing the mean and std of the prices offered weighted by how many times there were visited in MCTS tree exploration. This works well but is of limited use when all the possible prices for a given item are forbidden.

This could occur when all are greater than the amount of cash the offering player has. (This, however, was prevented by forcing the prices to be less than this). It could also occur when all of an item has been sold. This would, of course, make any offer impossible and produce a divide be zero exception when computing weighted means. How should this be resolved?

The question is really, "What produces the most useful training data for the NN? What mean/std of price do we want the NN to learn?" I'm not sure there is a good answer here. We don't care what price it learns. The mean and std are irrelevant and will not be used. Perhaps there is some way to include a value (NaN?) in a dataset which will not influence NN training on backpropagation? i.e. it always matches the trial output of the NN regardless of value. We could also use random values. Or simply put back in the trial mean/std. The simplest solution is just to let set N=1 for each price all prices are N=0 and let mean/std come out where they will. For now, I will proceed with this final option. The most important thing is really for the NN to learn to assign 0 weight to that item.

When generating training data, I use a single game state and AI for both players. This works because the players are identical and would make identical decisions. When running trial games, I have to use two separate game states because the players are different AIs and will explore different paths. In games with fixed choices which are all expanded as children nodes during exploration (like connect 4), the game states may be carried forward after the other player makes a move. This is because, all moves which P1 investigated are the same moves (and hence board states) that P2 investigated and chose from. This is not true for the AuctionGame. Since different AI will select different ranges of prices to offer, their future board states differ from those of their opponents. This means that after P1 selects a move, P2 must discard all its explorations and begin again on the new game state which P1's move creates. This will probably slow down the game.

See if the losses for different outputs need (and can) be scaled so that they are equally important in guiding training. These can be set using the loss_weights parameter in compile. The issue is that I don't know a priori what weights to use. Perhaps they can be estimated. You can also set the weights as variables and change them via a callback during training (https://github.com/keras-team/keras/issues/2595). In this way you could train with weights of 1 for the first n epochs and then normalize using, say, the average loss of the first n epochs.
This is a good idea but changing the weights during training produces a few problems. The main one is that in order to change the weights, they must be of the keras.backend.variable class. These are not JSON serializable and so when the model is stored, an error is thrown. I fixed this by changing the keras code using this discussion (https://github.com/keras-team/keras/issues/9342). I needed to modify it slightly to specifically catch tensorFlow RefVariables.
There is an additional problem that I have fixed although I don't understand the underlying reason for the bug. When a model is saved to file and then reloaded, it must be compiled before the loss_weights can be adjusted during training.